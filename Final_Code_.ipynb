{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+XuQGg6kDfHj188r1TGVB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamalmalikofficial/AIMS-softwarework/blob/main/Final_Code_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Run this script to automatically download all the required dependencies: ðŸ‘‡"
      ],
      "metadata": {
        "id": "dDB1obm-8Nx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1usW6cF2ntO4WylTYpdQJwFQx7sD_XrM-\n",
        "!gdown \"https://drive.google.com/uc?id=1e2-4xXQKW5H690YEIkgkCmChxe2o8U_B\"\n",
        "\n"
      ],
      "metadata": {
        "id": "NT0kEylu_LEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running this Script will take around 2 min ðŸ‘‡\n",
        "\n"
      ],
      "metadata": {
        "id": "7tsNFBicbV2w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iIEHCgtJorH"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git ftfy regex tqdm\n",
        "!pip install pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After all, run this main Script (the Driver code is at the end)"
      ],
      "metadata": {
        "id": "oTi26Lq2bmxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------\n",
        "# Config (basic settings)\n",
        "# -------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# ^ Use GPU if available, otherwise CPU\n",
        "\n",
        "DETECTOR_SCORE_THRESH = 0.1  # Only keep detected objects with confidence\n",
        "CLIP_SIM_THRESH = 0.20       # Only keep boxes where CLIP similarity\n",
        "TOP_K = 2                    # Keep only top 2 best fits\n",
        "CLIP_MODEL = \"ViT-B/32\"      # CLIP model version to use\n",
        "\n",
        "# -------------------------\n",
        "# Function to load an image from file\n",
        "def load_image(path):\n",
        "    return Image.open(path).convert(\"RGB\")\n",
        "\n",
        "# Function to draw boxes on an image\n",
        "def draw_boxes_pil(image, boxes, labels=None, scores=None, width=3):\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"DejaVuSans.ttf\", 16)  # Use nice font if available\n",
        "    except:\n",
        "        font = ImageFont.load_default()  # Fallback font\n",
        "\n",
        "    # Loop through each box and draw it\n",
        "    for i, box in enumerate(boxes):\n",
        "        x1, y1, x2, y2 = box\n",
        "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=width)  # Red rectangle\n",
        "\n",
        "        text = \"\"\n",
        "        if labels is not None:\n",
        "            text += str(labels[i])\n",
        "        if scores is not None:\n",
        "            text += f\" {scores[i]:.2f}\"\n",
        "\n",
        "        # Draw the label text above the box\n",
        "        if text:\n",
        "            if hasattr(draw, \"textbbox\"):  # For newer Pillow versions\n",
        "                tw, th = draw.textbbox((0, 0), text, font=font)[2:]\n",
        "            else:  # For older Pillow\n",
        "                tw, th = font.getsize(text)\n",
        "            draw.rectangle([x1, y1 - th, x1 + tw, y1], fill=\"red\")  # Red background\n",
        "            draw.text((x1, y1 - th), text, fill=\"white\", font=font)  # White text\n",
        "\n",
        "    return image\n",
        "\n",
        "# -------------------------\n",
        "# Load CLIP model (used to match text with image)\n",
        "print(\"Loading CLIP...\")\n",
        "clip_model, clip_preprocess = clip.load(CLIP_MODEL, device=device, jit=False)\n",
        "clip_model.eval()  # Evaluation mode (no training)\n",
        "\n",
        "# -------------------------\n",
        "# Load object detection model (Faster R-CNN)\n",
        "print(\"Loading detector (custom Faster R-CNN)...\")\n",
        "\n",
        "# Number of classes in your dataset (including background)\n",
        "num_classes = 13\n",
        "\n",
        "# Create model with same structure as trained one\n",
        "detector = fasterrcnn_resnet50_fpn(weights=None, num_classes=num_classes)\n",
        "\n",
        "# Load trained weights\n",
        "model_path = \"/content/fasterrcnn_custom.pth\"\n",
        "detector.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "detector.to(device)\n",
        "detector.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Transform: convert image to tensor for model\n",
        "detector_transform = T.Compose([\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# Main function: detect, crop, and match objects to a prompt\n",
        "def find_and_crop_multiple(image_pil, prompt, visualize=True, top_k=2):\n",
        "    # Convert image for detector\n",
        "    img_tensor = detector_transform(image_pil).to(device)\n",
        "    with torch.no_grad():  # No training, just detection\n",
        "        det_out = detector([img_tensor])[0]\n",
        "\n",
        "    boxes = det_out['boxes'].cpu()      # Object coordinates\n",
        "    det_scores = det_out['scores'].cpu()  # Confidence scores\n",
        "\n",
        "    # Filter by confidence score\n",
        "    keep_idx = (det_scores >= DETECTOR_SCORE_THRESH).nonzero(as_tuple=False).squeeze(1)\n",
        "    if keep_idx.numel() == 0:\n",
        "        print(\"No detector boxes above threshold.\")\n",
        "        return [], [], []\n",
        "\n",
        "    boxes = boxes[keep_idx].numpy()  # Keep only boxes above threshold\n",
        "\n",
        "    # Crop each detected object from image\n",
        "    crops = []\n",
        "    crop_boxes = []\n",
        "    for b in boxes:\n",
        "        x1, y1, x2, y2 = list(map(int, b))\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            continue  # Skip invalid boxes\n",
        "        crop = image_pil.crop((x1, y1, x2, y2)).convert(\"RGB\")\n",
        "        crops.append(crop)\n",
        "        crop_boxes.append((x1, y1, x2, y2))\n",
        "\n",
        "    if len(crops) == 0:\n",
        "        print(\"No valid crops found.\")\n",
        "        return [], [], []\n",
        "\n",
        "    # Prepare crops for CLIP\n",
        "    clip_inputs = torch.stack([clip_preprocess(crop) for crop in crops]).to(device)\n",
        "    text_tokens = clip.tokenize([prompt]).to(device)\n",
        "\n",
        "    # Get similarity scores between crops and the text\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(clip_inputs)\n",
        "        text_features = clip_model.encode_text(text_tokens)\n",
        "\n",
        "        # Normalize features so they can be compared\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        sims = (image_features @ text_features.T).squeeze(1).cpu().numpy()\n",
        "\n",
        "    # Pick top_k crops with highest similarity\n",
        "    top_indices = np.argsort(-sims)[:top_k]\n",
        "    best_crops = [crops[i] for i in top_indices]\n",
        "    best_boxes = [crop_boxes[i] for i in top_indices]\n",
        "    best_scores = [sims[i] for i in top_indices]\n",
        "\n",
        "    # If visualize=True, show the best matches\n",
        "    if visualize:\n",
        "        for idx, crop in enumerate(best_crops):\n",
        "            plt.figure(figsize=(5, 5))\n",
        "            plt.imshow(crop)\n",
        "            plt.title(f\"{prompt} ({best_scores[idx]:.2f})\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "\n",
        "    return best_crops, best_boxes, best_scores\n",
        "\n",
        "# -------------------------\n",
        "# Example usage / Driver Code\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"17.jpg\"  # Path to input image\n",
        "    image = load_image(image_path)\n",
        "\n",
        "    # Display the original image\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    prompt = \" human on bicycle \"  # Main Prompt\n",
        "    crops, boxes, scores = find_and_crop_multiple(image, prompt, visualize=True, top_k=2)\n",
        "\n",
        "    # Save the best crops to files\n",
        "    if crops:\n",
        "        for i, c in enumerate(crops):\n",
        "            c.save(f\"best_crop_{i+1}.png\")\n",
        "            print(f\"Saved best_crop_{i+1}.png with score {scores[i]:.3f}\")\n"
      ],
      "metadata": {
        "id": "bFVP9A1XPcKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fyh8Irud9nmY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}