{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTY5T+Xyq/c+iKjdtB3MZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamalmalikofficial/AIMS-softwarework/blob/main/retraining_OWLvit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fJnkSwJJKeOz",
        "outputId": "39e41e26-99dc-4d49-ce61-86f91feaa5f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "f9a082942e3f4052b4ed13421c7364d6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install torch torchvision transformers pycocotools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers pycocotools"
      ],
      "metadata": {
        "id": "CyJhD88lNEa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7629e40-9497-4205-c0e4-494629039204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
        "\n",
        "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "model.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUfs_QD006ij",
        "outputId": "7cb4db29-6bc3-471a-b882-fa2bdf7e811e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OwlViTForObjectDetection(\n",
              "  (owlvit): OwlViTModel(\n",
              "    (text_model): OwlViTTextTransformer(\n",
              "      (embeddings): OwlViTTextEmbeddings(\n",
              "        (token_embedding): Embedding(49408, 512)\n",
              "        (position_embedding): Embedding(16, 512)\n",
              "      )\n",
              "      (encoder): OwlViTEncoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x OwlViTEncoderLayer(\n",
              "            (self_attn): OwlViTAttention(\n",
              "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): OwlViTMLP(\n",
              "              (activation_fn): QuickGELUActivation()\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            )\n",
              "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (vision_model): OwlViTVisionTransformer(\n",
              "      (embeddings): OwlViTVisionEmbeddings(\n",
              "        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "        (position_embedding): Embedding(577, 768)\n",
              "      )\n",
              "      (pre_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (encoder): OwlViTEncoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x OwlViTEncoderLayer(\n",
              "            (self_attn): OwlViTAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): OwlViTMLP(\n",
              "              (activation_fn): QuickGELUActivation()\n",
              "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              "  )\n",
              "  (class_head): OwlViTClassPredictionHead(\n",
              "    (dense0): Linear(in_features=768, out_features=512, bias=True)\n",
              "    (logit_shift): Linear(in_features=768, out_features=1, bias=True)\n",
              "    (logit_scale): Linear(in_features=768, out_features=1, bias=True)\n",
              "    (elu): ELU(alpha=1.0)\n",
              "  )\n",
              "  (box_head): OwlViTBoxPredictionHead(\n",
              "    (dense0): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dense1): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (gelu): GELU(approximate='none')\n",
              "    (dense2): Linear(in_features=768, out_features=4, bias=True)\n",
              "  )\n",
              "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v66PK2TU4voJ",
        "outputId": "8228b49e-7eb5-42cc-f0b6-3f210d60b95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NkJQGtBd5ZlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision import transforms\n",
        "\n",
        "images_root = \"/content/drive/MyDrive/images\"\n",
        "annotations_file = \"/content/result.json\"  # your COCO JSON file\n",
        "\n",
        "# No transform here, keep images as PIL for processor\n",
        "coco_dataset = CocoDetection(root=images_root, annFile=annotations_file, transform=None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1pnuKhI1aoa",
        "outputId": "b8b1310d-d500-447c-eac3-10ab5682b41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CocoFruitDataset(Dataset):\n",
        "    def __init__(self, coco_json_path, images_dir, processor):\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.processor = processor\n",
        "\n",
        "        with open(coco_json_path, \"r\") as f:\n",
        "            coco = json.load(f)\n",
        "\n",
        "        self.images_info = coco[\"images\"]\n",
        "        self.annotations = coco[\"annotations\"]\n",
        "\n",
        "        # Map image_id → annotations\n",
        "        self.ann_by_img = {}\n",
        "        for ann in self.annotations:\n",
        "            self.ann_by_img.setdefault(ann[\"image_id\"], []).append(ann)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_info = self.images_info[idx]\n",
        "        image_id = img_info[\"id\"]\n",
        "\n",
        "        # Always use only the filename\n",
        "        filename = Path(img_info[\"file_name\"]).name\n",
        "        img_path = self.images_dir / filename\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        anns = self.ann_by_img.get(image_id, [])\n",
        "\n",
        "        # Convert COCO bbox format [x, y, w, h] → [x_min, y_min, x_max, y_max]\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(ann[\"category_id\"])\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        return image, target\n",
        "\n",
        "# Example usage:\n",
        "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "dataset = CocoFruitDataset(\n",
        "    coco_json_path=\"/content/result.json\",\n",
        "    images_dir=\"/content/drive/MyDrive/images\",\n",
        "    processor=processor\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n"
      ],
      "metadata": {
        "id": "QdVVTGFX6tin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# owlvit_finetune_colab.py\n",
        "# Run in Colab (mount Drive if needed) or local env with GPU.\n",
        "# Requirements:\n",
        "# pip install torch torchvision transformers accelerate\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import json\n",
        "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset\n",
        "# ---------------------------\n",
        "class CocoFruitDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Expects COCO-format JSON with 'images', 'annotations', and 'categories'.\n",
        "    Each __getitem__ returns (PIL_image, text_labels, boxes_tensor).\n",
        "    text_labels is a list of strings for that image, e.g. [\"a photo of an apple\", \"a photo of a banana\"]\n",
        "    boxes_tensor is Nx4 tensor in [x_min, y_min, x_max, y_max] (absolute pixel coords).\n",
        "    \"\"\"\n",
        "    def __init__(self, coco_json_path, images_dir):\n",
        "        self.images_dir = Path(images_dir)\n",
        "        with open(coco_json_path, \"r\") as f:\n",
        "            coco = json.load(f)\n",
        "        self.images_info = coco[\"images\"]\n",
        "        self.annotations = coco[\"annotations\"]\n",
        "        self.categories = {cat[\"id\"]: cat[\"name\"] for cat in coco.get(\"categories\", [])}\n",
        "\n",
        "        # map image_id -> list of annotations\n",
        "        self.ann_by_img = {}\n",
        "        for ann in self.annotations:\n",
        "            self.ann_by_img.setdefault(ann[\"image_id\"], []).append(ann)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_info = self.images_info[idx]\n",
        "        image_id = img_info[\"id\"]\n",
        "        filename = Path(img_info[\"file_name\"]).name\n",
        "        img_path = self.images_dir / filename\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        anns = self.ann_by_img.get(image_id, [])\n",
        "        boxes = []\n",
        "        text_labels = []\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            cat_name = self.categories.get(ann[\"category_id\"], str(ann[\"category_id\"]))\n",
        "            # OWL-ViT expects natural language prompts; use a short prompt template:\n",
        "            text_labels.append(f\"a photo of a {cat_name}\")\n",
        "\n",
        "        # if no annotations, give placeholder empty lists (model can still be trained on such images)\n",
        "        boxes_tensor = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,4), dtype=torch.float32)\n",
        "\n",
        "        return image, text_labels, boxes_tensor, (image.height, image.width)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Collate function using processor\n",
        "# ---------------------------\n",
        "def make_collate_fn(processor: OwlViTProcessor):\n",
        "    \"\"\"\n",
        "    processor expects:\n",
        "      text: list[list[str]]  (a list of list-of-queries per image)\n",
        "      images: list[PIL.Image]\n",
        "    We return the dict from processor (pixel_values, input_ids, attention_mask, ...) and also\n",
        "    'gt_boxes' (list of tensors) and 'original_sizes' for post-processing.\n",
        "    \"\"\"\n",
        "    def collate(batch):\n",
        "        images, text_labels_batch, boxes_batch, sizes_batch = zip(*batch)\n",
        "        # text_labels_batch should be a list of list[str] (even if a single query per image)\n",
        "        # Processor will tokenize/pad the nested list automatically.\n",
        "        processor_inputs = processor(text=list(text_labels_batch), images=list(images), return_tensors=\"pt\")\n",
        "        # Add ground truth boxes (keep as list so we can compute losses per-sample)\n",
        "        processor_inputs[\"gt_boxes\"] = boxes_batch  # list of (N_i,4) tensors in absolute pixel coords\n",
        "        processor_inputs[\"orig_sizes\"] = torch.tensor(sizes_batch)  # (H,W) per image\n",
        "        return processor_inputs\n",
        "    return collate\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Simple helper: IoU + greedy matching\n",
        "# ---------------------------\n",
        "def box_iou(boxes1, boxes2):\n",
        "    # boxes: (N,4) [x1,y1,x2,y2]\n",
        "    if boxes1.numel() == 0 or boxes2.numel() == 0:\n",
        "        return torch.zeros((boxes1.shape[0], boxes2.shape[0]), device=boxes1.device)\n",
        "    x1 = torch.max(boxes1[:, None, 0], boxes2[None, :, 0])\n",
        "    y1 = torch.max(boxes1[:, None, 1], boxes2[None, :, 1])\n",
        "    x2 = torch.min(boxes1[:, None, 2], boxes2[None, :, 2])\n",
        "    y2 = torch.min(boxes1[:, None, 3], boxes2[None, :, 3])\n",
        "    inter = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n",
        "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
        "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
        "    union = area1[:, None] + area2[None, :] - inter\n",
        "    return inter / (union + 1e-8)\n",
        "\n",
        "def greedy_match(pred_boxes, gt_boxes):\n",
        "    # returns matched pairs (pred_idx, gt_idx)\n",
        "    if pred_boxes.numel() == 0 or gt_boxes.numel() == 0:\n",
        "        return []\n",
        "    ious = box_iou(pred_boxes, gt_boxes)  # (P, G)\n",
        "    matches = []\n",
        "    ious_clone = ious.clone()\n",
        "    while True:\n",
        "        max_val, idx = ious_clone.view(-1).max(0)\n",
        "        if max_val <= 0:\n",
        "            break\n",
        "        p = int(idx // ious_clone.shape[1])\n",
        "        g = int(idx % ious_clone.shape[1])\n",
        "        matches.append((p, g))\n",
        "        ious_clone[p, :] = -1\n",
        "        ious_clone[:, g] = -1\n",
        "        if (ious_clone <= 0).all():\n",
        "            break\n",
        "    return matches\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Training loop (simple)\n",
        "# ---------------------------\n",
        "def train(\n",
        "    train_dataset,\n",
        "    model,\n",
        "    processor,\n",
        "    save_dir=\"./owlvit_checkpoints\",\n",
        "    epochs=3,\n",
        "    batch_size=2,\n",
        "    lr=5e-5,\n",
        "    device=None,\n",
        "    freeze_backbone=True\n",
        "):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    if freeze_backbone:\n",
        "        # freeze vision/text encoders to train only detection heads initially\n",
        "        for name, p in model.named_parameters():\n",
        "            if \"detection_head\" not in name and \"proj\" not in name and \"box\" not in name:\n",
        "                p.requires_grad = False\n",
        "\n",
        "    collate_fn = make_collate_fn(processor)\n",
        "    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            # processor returns tensors: pixel_values (B, C, H, W), input_ids (B, L), attention_mask (B, L)\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            input_ids = batch.get(\"input_ids\")\n",
        "            attention_mask = batch.get(\"attention_mask\")\n",
        "            if input_ids is not None:\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "\n",
        "            gt_boxes_list = batch[\"gt_boxes\"]  # list of (Ni,4) tensors (absolute coords)\n",
        "            orig_sizes = batch[\"orig_sizes\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
        "                model_inputs = {\"pixel_values\": pixel_values}\n",
        "                if input_ids is not None:\n",
        "                    model_inputs[\"input_ids\"] = input_ids\n",
        "                    model_inputs[\"attention_mask\"] = attention_mask\n",
        "\n",
        "                outputs = model(**model_inputs)\n",
        "                # outputs is OwlViTObjectDetectionOutput; typical fields include logits and pred_boxes (normalized)\n",
        "                pred_boxes = outputs.pred_boxes  # shape (B, num_queries, 4) in normalized [0..1]\n",
        "                # Convert normalized to absolute coords for matching\n",
        "                bsz = pred_boxes.shape[0]\n",
        "                abs_pred_boxes_list = []\n",
        "                for i in range(bsz):\n",
        "                    h, w = orig_sizes[i]\n",
        "                    pb = pred_boxes[i]\n",
        "                    # pred format: [cx, cy, w, h] or normalized (depends on model) - HF docs show post_process expects normalized [cx,cy,w,h]\n",
        "                    # Convert center-format [cx,cy,w,h] -> [x_min,y_min,x_max,y_max]\n",
        "                    cxcywh = pb\n",
        "                    cx = cxcywh[:, 0] * w\n",
        "                    cy = cxcywh[:, 1] * h\n",
        "                    ww = cxcywh[:, 2] * w\n",
        "                    hh = cxcywh[:, 3] * h\n",
        "                    x1 = cx - 0.5 * ww\n",
        "                    y1 = cy - 0.5 * hh\n",
        "                    x2 = cx + 0.5 * ww\n",
        "                    y2 = cy + 0.5 * hh\n",
        "                    abs_pred = torch.stack([x1, y1, x2, y2], dim=1)\n",
        "                    abs_pred_boxes_list.append(abs_pred)\n",
        "\n",
        "                # compute a simple box L1 loss using greedy matching (prototype)\n",
        "                loss = torch.tensor(0.0, device=device)\n",
        "                count = 0\n",
        "                for i in range(bsz):\n",
        "                    pred_abs = abs_pred_boxes_list[i]\n",
        "                    gt = gt_boxes_list[i].to(device)\n",
        "                    if gt.numel() == 0:\n",
        "                        continue\n",
        "                    matches = greedy_match(pred_abs.detach(), gt.detach())\n",
        "                    if not matches:\n",
        "                        # no match: penalize closest predicted box L1 to first gt (helps learning)\n",
        "                        chosen_pred = pred_abs[0:1]\n",
        "                        chosen_gt = gt[0:1]\n",
        "                        loss = loss + F.l1_loss(chosen_pred, chosen_gt, reduction=\"sum\")\n",
        "                        count += chosen_gt.numel()\n",
        "                        continue\n",
        "                    for p_idx, g_idx in matches:\n",
        "                        loss = loss + F.l1_loss(pred_abs[p_idx:p_idx+1], gt[g_idx:g_idx+1], reduction=\"sum\")\n",
        "                        count += 4\n",
        "                if count > 0:\n",
        "                    loss = loss / count\n",
        "                else:\n",
        "                    loss = loss  # 0\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix({\"loss\": f\"{epoch_loss/(pbar.n+1):.4f}\"})\n",
        "\n",
        "        # end epoch\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1} finished. Avg loss: {avg_loss:.6f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        ckpt_path = os.path.join(save_dir, f\"owlvit_epoch{epoch+1}.pt\")\n",
        "        torch.save({\"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, ckpt_path)\n",
        "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Usage / main\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Paths - change to your own\n",
        "    COCO_JSON = \"/content/result.json\"\n",
        "    IMAGES_DIR = \"/content/drive/MyDrive/images\"\n",
        "\n",
        "    processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "    model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "\n",
        "    dataset = CocoFruitDataset(coco_json_path=COCO_JSON, images_dir=IMAGES_DIR)\n",
        "\n",
        "    # Quick test: single sample through processor (debug)\n",
        "    img, txts, boxes, size = dataset[0]\n",
        "    print(\"sample text labels:\", txts)\n",
        "    # Note: processor expects nested list of queries per image\n",
        "    test_inputs = processor(text=[txts], images=img, return_tensors=\"pt\")\n",
        "    print(\"processor keys:\", test_inputs.keys())\n",
        "\n",
        "    # Train (small example)\n",
        "    train(dataset, model, processor, save_dir=\"./owlvit_checkpoints\", epochs=3, batch_size=2, lr=3e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "fbww0SDd-h33",
        "outputId": "2a320f43-805f-4416-cdcf-50fe7016cfc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/images/..\\\\..\\\\label-studio\\\\label-studio\\\\media\\\\upload\\\\8\\\\bdc4ef67-img97.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1276182203.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;31m# Quick test: single sample through processor (debug)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample text labels:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Note: processor expects nested list of queries per image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1276182203.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0manns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mann_by_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/images/..\\\\..\\\\label-studio\\\\label-studio\\\\media\\\\upload\\\\8\\\\bdc4ef67-img97.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "imp92bqzdMcb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}